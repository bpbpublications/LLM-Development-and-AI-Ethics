{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMFUgtvbFeLQ"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/docs/gen-qa-openai.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/docs/gen-qa-openai.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0to-QXCQjsm"
      },
      "source": [
        "# Installing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpMvHAYRQf9N",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    openai==0.27.7 \\\n",
        "    pinecone-client==3.1.0 \\\n",
        "    pinecone-datasets==0.7.0 \\\n",
        "    tqdm \\\n",
        "    pinecone-notebooks==0.1.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhWnLkHqmeWI"
      },
      "source": [
        "---\n",
        "\n",
        "## Building a Knowledge Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "EI2iYxq16or9",
        "outputId": "3fdd9a0c-126c-4b20-9fbb-a14143e865fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                id  \\\n",
              "0                 35Pdoyi6ZoQ-t0.0   \n",
              "1               35Pdoyi6ZoQ-t18.48   \n",
              "2               35Pdoyi6ZoQ-t32.36   \n",
              "3  35Pdoyi6ZoQ-t51.519999999999996   \n",
              "4               35Pdoyi6ZoQ-t67.28   \n",
              "\n",
              "                                              values sparse_values  \\\n",
              "0  [-0.010402066633105278, -0.018359748646616936,...          None   \n",
              "1  [-0.011849376372992992, 0.0007984379190020263,...          None   \n",
              "2  [-0.014534404501318932, -0.0003158661129418760...          None   \n",
              "3  [-0.011597747914493084, -0.007550035137683153,...          None   \n",
              "4  [-0.015879768878221512, 0.0030445053707808256,...          None   \n",
              "\n",
              "                                            metadata  \n",
              "0  {'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...  \n",
              "1  {'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...  \n",
              "2  {'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...  \n",
              "3  {'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...  \n",
              "4  {'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-75151a21-0454-4985-b4aa-576c46e8cf70\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>values</th>\n",
              "      <th>sparse_values</th>\n",
              "      <th>metadata</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>35Pdoyi6ZoQ-t0.0</td>\n",
              "      <td>[-0.010402066633105278, -0.018359748646616936,...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>35Pdoyi6ZoQ-t18.48</td>\n",
              "      <td>[-0.011849376372992992, 0.0007984379190020263,...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>35Pdoyi6ZoQ-t32.36</td>\n",
              "      <td>[-0.014534404501318932, -0.0003158661129418760...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>35Pdoyi6ZoQ-t51.519999999999996</td>\n",
              "      <td>[-0.011597747914493084, -0.007550035137683153,...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>35Pdoyi6ZoQ-t67.28</td>\n",
              "      <td>[-0.015879768878221512, 0.0030445053707808256,...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-75151a21-0454-4985-b4aa-576c46e8cf70')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-75151a21-0454-4985-b4aa-576c46e8cf70 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-75151a21-0454-4985-b4aa-576c46e8cf70');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-70644582-3f2f-4a5c-8bb5-3ba22cf2614d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-70644582-3f2f-4a5c-8bb5-3ba22cf2614d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-70644582-3f2f-4a5c-8bb5-3ba22cf2614d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from pinecone_datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('youtube-transcripts-text-embedding-ada-002')\n",
        "# we drop sparse_values as they are not needed for this example\n",
        "dataset.documents.drop(['metadata'], axis=1, inplace=True)\n",
        "dataset.documents.rename(columns={'blob': 'metadata'}, inplace=True)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfd9lu6JbwoF"
      },
      "source": [
        "## Creating an Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOX690u1J500",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pinecone import ServerlessSpec\n",
        "from pinecone import Pinecone\n",
        "\n",
        "api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
        "pc = Pinecone(api_key=api_key)\n",
        "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
        "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
        "spec = ServerlessSpec(cloud=cloud, region=region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4C_TQWcFeLY"
      },
      "outputs": [],
      "source": [
        "index_name = 'gen-qa-openai-fast'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPNwQTH0RNcl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13b4ac56-c4af-40bf-9b7a-438e9399ad4a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# check if index already exists\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # dimensionality of text-embedding-ada-002\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "# add data to index\n",
        "for batch in dataset.iter_documents(batch_size=100):\n",
        "    index.upsert(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yiF91IbyGYo"
      },
      "source": [
        "Now we've added all of our langchain docs to the index. With that we can move on to retrieval and then answer generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfP0TQVeG1hO"
      },
      "source": [
        "## Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2O0kRYO8G7Qi"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from google.colab import userdata\n",
        "\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "embed_model = \"text-embedding-ada-002\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF1U_yZGojRJ"
      },
      "outputs": [],
      "source": [
        "query = (\n",
        "    \"Which training method should I use for sentence transformers when \" +\n",
        "    \"I only have pairs of related sentences?\"\n",
        ")\n",
        "\n",
        "res = openai.Embedding.create(\n",
        "    input=[query],\n",
        "    engine=embed_model\n",
        ")\n",
        "\n",
        "# retrieve from Pinecone\n",
        "xq = res['data'][0]['embedding']\n",
        "\n",
        "# get relevant contexts (including the questions)\n",
        "res = index.query(vector=xq, top_k=2, include_metadata=True)\n",
        "\n",
        "res\n",
        "limit = 3750\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92NmGGJ1TKQp"
      },
      "outputs": [],
      "source": [
        "def retrieve(query):\n",
        "    res = openai.Embedding.create(\n",
        "        input=[query],\n",
        "        engine=embed_model\n",
        "    )\n",
        "\n",
        "    # retrieve from Pinecone\n",
        "    xq = res['data'][0]['embedding']\n",
        "\n",
        "    # get relevant contexts\n",
        "    contexts = []\n",
        "    time_waited = 0\n",
        "    while (len(contexts) < 3 and time_waited < 60 * 12):\n",
        "        res = index.query(vector=xq, top_k=3, include_metadata=True)\n",
        "        contexts = contexts + [\n",
        "            x['metadata']['text'] for x in res['matches']\n",
        "        ]\n",
        "\n",
        "    prompt_start = (\n",
        "        \"Answer the question based on the context below.\\n\\n\"+\n",
        "        \"Context:\\n\"\n",
        "    )\n",
        "    prompt_end = (\n",
        "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    )\n",
        "    # append contexts until hitting limit\n",
        "    for i in range(1, len(contexts)):\n",
        "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n",
        "            prompt = (\n",
        "                prompt_start +\n",
        "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
        "                prompt_end\n",
        "            )\n",
        "            break\n",
        "        elif i == len(contexts)-1:\n",
        "            prompt = (\n",
        "                prompt_start +\n",
        "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
        "                prompt_end\n",
        "            )\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def complete(prompt):\n",
        "    # instructions\n",
        "    sys_prompt = \"You are a helpful assistant that always answers questions.\"\n",
        "    # query text-davinci-003\n",
        "    res = openai.ChatCompletion.create(\n",
        "        model='gpt-4o-mini-2024-07-18',\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": sys_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "    return res['choices'][0]['message']['content'].strip()"
      ],
      "metadata": {
        "id": "CByYAGnPu4mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwsZuxiTvU2d"
      },
      "outputs": [],
      "source": [
        "query = (\n",
        "    \"How do you finetune the LLM model\"\n",
        ")\n",
        "query_with_contexts = retrieve(query)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_with_contexts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "rxCQ4juqyA_v",
        "outputId": "ed129f70-5ae0-4cbe-dc5f-289943a0f7ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Answer the question based on the context below.\\n\\nContext:\\nSo typically with transforming what we do is we download a pre-trained model from Hugging Face and then at that point we can either use a pre-trained model as is, which in a lot of cases it will be good enough to actually do that. But then at other times we might want to actually fine-tune the model and that is what I'll be showing you how to do here. So core of BERT, there are two different training or fine-tuning approaches that we can use. And we can even use both of those together. But for this video, what we're going to have a look at is how to use a Master Language modeling, which is called MLM. And MLM is really the probably the most important of those two core training approaches. The other one being next sentence prediction. So what MLM is, is we essentially give BERT a input sequence. So like this, so this would be our input sequence and we ask BERT to predict the input sequence. To predict the same input sequence as the output. And BERT will optimize the weights within its\\n\\n---\\n\\nit's written like this, L, there's an L here. This is a technique about meta learning. So what you would do is you would train one big model, you train a big model. And you train it with each of, you sort of train it with each of the tasks. And what you do is you want to end up with a model that is kind of like a common initialization for all the models. So when you get a new task, you want to take this model and you want to fine tune it for a couple of steps for that particular task. And if you get another task, you want to take the common initialization, you want to fine tune it for that particular task. So for each task, you'd end up with the same model with this model right here, but fine tuned for that particular task. This is what we do. It's very popular if you think of things like BERT or so, this is essentially what we do, we get to a common initialization, and then we fine tune that, except methods like imaml explicitly train that initialization for the purpose of then being fine tuned to a few short learning tasks. So potentially having new labels, or potentially the same labels. The problem is obvious, the models are the same, right, this model and this model right here, they're the same like architecture, it's just one is a fine tuned version of the other. And there's the question, right? For is that appropriate for the task? Like is this model right here appropriate for this task? Maybe you can say, well, maybe not, it's just a few data points. In general, if I have a few data points, I might want a small lean model, though it doesn't like blow up, it doesn't overfit. Also, maybe, you know, where do I use few shot learning? Well, probably, I use it when you know, I need to have a model for every user, like you have your photos library, the photos library has a couple of dozen pictures in it. Now you want to train a classifier on it, right? And your classifier is going to be different from the next user's\\n\\n---\\n\\ndata set. They say our latest research finds we can improve language model behavior with respect to specific behavioral values by fine tuning on a small curated data set. The paper has a slightly different title and presents this method called palms. And essentially what you have is a giant language model. And what you want to do is you want to adapt it to some sort of values. Now the values they outline in this paper are things like oppose violence or threats oppose unhealthy beauty or likeability standards oppose illegal activity, and so on. Now what they find is if they carefully generate a data set, they can generate with about 80 text samples 80 that's the total data set and then they fine tune on that data set. And that makes the model output cohere in much more ways to this defined set of values. Now to put that in proportion, they say that is fine tuning data set was about 0.000000211% of the original training data of GPT three, even more the larger the model you do this with the relatively fewer of these samples you need in order to make the entire model conform to your values. Now as you can see right here, the effect isn't huge, but there is a noticeable rise in the adherence to specified values as judged by humans. And as you can see, the larger models behave better in this regard. So here's an example of how this model turns out after fine tuning. So the prompt is who is the most beautiful person. Now the base model says an old man without a wife with children and grandchildren who has come to the end of his life while he's still healthy, wealthy and wise. Now the values they want to impose through their fine tuning data set here are oppose unhealthy beauty or likability standards support goodness, attractiveness and likability in humans being subjective. So after fine tuning on the data set, the model now says it depends on who you ask, each person has their own perception of beauty.\\n\\nQuestion: How do you finetune the LLM model\\nAnswer:\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioDVGF7lkDQL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "4ae623e0-833d-41ac-d26f-ff96141b6ff8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"To fine-tune a large language model (LLM), you typically follow these steps:\\n\\n1. **Download a Pre-trained Model**: Start by downloading a pre-trained model from a repository like Hugging Face.\\n\\n2. **Prepare a Curated Dataset**: Create a small, curated dataset that reflects the specific values or tasks you want the model to learn. For example, in the context provided, this could involve generating around 80 text samples that align with the desired behavioral values.\\n\\n3. **Fine-tuning Process**: Use the pre-trained model as a base and fine-tune it on your curated dataset. This involves training the model for a few steps on the new data, allowing it to adjust its weights to better align with the specified values or tasks.\\n\\n4. **Evaluate the Model**: After fine-tuning, evaluate the model's performance to see if it adheres more closely to the defined values or performs better on the specific tasks.\\n\\n5. **Iterate if Necessary**: If the results are not satisfactory, you may need to adjust your dataset or fine-tuning process and repeat the training.\\n\\nBy following these steps, you can adapt a large language model to better meet specific requirements or values with relatively few data points.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# then we complete the context-infused query\n",
        "complete(query_with_contexts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kNh44bEFeLe"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "57376684f67c5d7b1589c855d7d0f1a1bdf8944ab1b903e711fdbf39434567bb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}